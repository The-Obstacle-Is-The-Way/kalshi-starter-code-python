# Remaining Work - Truth vs. Claims Audit

**Audit Date:** 2026-01-07
**Auditor:** Claude (Opus 4.5) - Independent verification
**Status:** PRECISE SPECS FOR SENIOR IMPLEMENTATION

---

# âš ï¸ CRITICAL FINDING: Mock Data in Production Code

**This is reward hacking. The code looks complete but delivers zero value.**

## Issue #1: Backtest CLI Outputs Fake Data (ðŸ”´ CRITICAL)

### The Problem

**File:** `src/kalshi_research/cli.py`
**Lines:** 1615-1645

The `kalshi research backtest` command accepts parameters but **ignores them entirely** and outputs hardcoded fake results:

```python
@research_app.command("backtest")
def research_backtest(
    start: Annotated[str, typer.Option("--start", help="Start date (YYYY-MM-DD)")],
    end: Annotated[str, typer.Option("--end", help="End date (YYYY-MM-DD)")],
    db_path: Annotated[Path, typer.Option("--db", "-d", ...)],
) -> None:
    """Run a backtest (placeholder - requires strategy implementation)."""
    # ... validation that looks legit ...

    # Mock output for now  <-- THE SMOKING GUN
    table = Table(title="Backtest Results")
    table.add_row("Total Trades", "10")       # HARDCODED FAKE
    table.add_row("Win Rate", "60.0%")        # HARDCODED FAKE
    table.add_row("Total P&L", "$150.00")     # HARDCODED FAKE
    table.add_row("Sharpe Ratio", "1.5")      # HARDCODED FAKE
    console.print(table)
```

### Why This Is Egregious

1. **The `ThesisBacktester` class EXISTS and WORKS** in `src/kalshi_research/research/backtest.py`
2. Parameters `start`, `end`, `db_path` are accepted but IGNORED
3. User thinks they're running a backtest but getting fake results
4. Trading decisions could be made on this fake data

### The Exact Fix

**Replace lines 1615-1645 in `src/kalshi_research/cli.py` with:**

```python
@research_app.command("backtest")
def research_backtest(
    start: Annotated[str, typer.Option("--start", help="Start date (YYYY-MM-DD)")],
    end: Annotated[str, typer.Option("--end", help="End date (YYYY-MM-DD)")],
    thesis_id: Annotated[
        str | None,
        typer.Option("--thesis", "-t", help="Specific thesis ID to backtest (default: all resolved)"),
    ] = None,
    db_path: Annotated[
        Path,
        typer.Option("--db", "-d", help="Path to SQLite database file."),
    ] = Path("data/kalshi.db"),
) -> None:
    """
    Run backtests on resolved theses using historical settlements.

    Uses the ThesisBacktester class to compute real P&L, win rate, and Brier scores
    from actual settlement data in the database.

    Examples:
        kalshi research backtest --start 2024-01-01 --end 2024-12-31
        kalshi research backtest --thesis abc123 --start 2024-06-01 --end 2024-12-31
    """
    from datetime import datetime

    from sqlalchemy import select

    from kalshi_research.data import DatabaseManager
    from kalshi_research.data.models import Settlement
    from kalshi_research.research.backtest import ThesisBacktester
    from kalshi_research.research.thesis import ThesisManager, ThesisStatus

    if not db_path.exists():
        console.print(f"[red]Error:[/red] Database not found at {db_path}")
        console.print("[dim]Run 'kalshi data init' first.[/dim]")
        raise typer.Exit(1)

    async def _backtest() -> None:
        db = DatabaseManager(db_path)
        thesis_mgr = ThesisManager()
        backtester = ThesisBacktester()

        try:
            # Parse date range
            try:
                start_dt = datetime.fromisoformat(start)
                end_dt = datetime.fromisoformat(end)
            except ValueError as e:
                console.print(f"[red]Error:[/red] Invalid date format: {e}")
                console.print("[dim]Use YYYY-MM-DD format.[/dim]")
                raise typer.Exit(1) from None

            if start_dt >= end_dt:
                console.print("[red]Error:[/red] Start date must be before end date")
                raise typer.Exit(1)

            # Load theses
            if thesis_id:
                thesis = thesis_mgr.get(thesis_id)
                if not thesis:
                    console.print(f"[red]Error:[/red] Thesis '{thesis_id}' not found")
                    console.print("[dim]Use 'kalshi research thesis list' to see available theses.[/dim]")
                    raise typer.Exit(1)
                theses = [thesis]
            else:
                theses = thesis_mgr.list_all()

            # Filter to resolved theses only
            resolved = [t for t in theses if t.status == ThesisStatus.RESOLVED]
            if not resolved:
                console.print("[yellow]No resolved theses to backtest[/yellow]")
                console.print("[dim]Theses must be resolved before backtesting. Use 'kalshi research thesis resolve'.[/dim]")
                return

            console.print(f"[dim]Found {len(resolved)} resolved theses[/dim]")

            # Load settlements from DB
            async with db.session_factory() as session:
                result = await session.execute(
                    select(Settlement).where(
                        Settlement.settled_at >= start_dt,
                        Settlement.settled_at <= end_dt,
                    )
                )
                settlements = list(result.scalars().all())

            if not settlements:
                console.print(f"[yellow]No settlements found between {start} and {end}[/yellow]")
                console.print("[dim]Run 'kalshi data sync-settlements' to fetch settlement data.[/dim]")
                return

            console.print(f"[dim]Found {len(settlements)} settlements in date range[/dim]")

            # Run backtest with progress indicator
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                transient=True,
            ) as progress:
                progress.add_task(f"Backtesting {len(resolved)} theses...", total=None)
                results = await backtester.backtest_all(resolved, settlements)

            if not results:
                console.print("[yellow]No backtest results generated[/yellow]")
                console.print("[dim]This can happen if no theses match the settlement data.[/dim]")
                return

            # Calculate aggregate statistics
            total_trades = sum(r.total_trades for r in results)
            total_pnl = sum(r.total_pnl for r in results)
            total_wins = sum(r.winning_trades for r in results)
            avg_brier = sum(r.brier_score for r in results) / len(results) if results else 0

            # Display summary table
            summary_table = Table(title="Backtest Summary")
            summary_table.add_column("Metric", style="cyan")
            summary_table.add_column("Value", style="green")

            summary_table.add_row("Date Range", f"{start} to {end}")
            summary_table.add_row("Theses Tested", str(len(results)))
            summary_table.add_row("Total Trades", str(total_trades))
            summary_table.add_row(
                "Aggregate Win Rate",
                f"{total_wins / total_trades:.1%}" if total_trades > 0 else "N/A",
            )
            pnl_color = "green" if total_pnl >= 0 else "red"
            summary_table.add_row("Total P&L", f"[{pnl_color}]{total_pnl:+.0f}Â¢[/{pnl_color}]")
            summary_table.add_row("Avg Brier Score", f"{avg_brier:.4f}")

            console.print(summary_table)
            console.print()

            # Display per-thesis results
            detail_table = Table(title="Per-Thesis Results")
            detail_table.add_column("Thesis ID", style="cyan", max_width=15)
            detail_table.add_column("Trades", justify="right")
            detail_table.add_column("Win Rate", justify="right")
            detail_table.add_column("P&L", justify="right")
            detail_table.add_column("Brier", justify="right")
            detail_table.add_column("Sharpe", justify="right")

            for result in sorted(results, key=lambda r: r.total_pnl, reverse=True):
                pnl_str = f"{result.total_pnl:+.0f}Â¢"
                pnl_color = "green" if result.total_pnl >= 0 else "red"
                detail_table.add_row(
                    result.thesis_id[:15],
                    str(result.total_trades),
                    f"{result.win_rate:.1%}" if result.total_trades > 0 else "N/A",
                    f"[{pnl_color}]{pnl_str}[/{pnl_color}]",
                    f"{result.brier_score:.4f}",
                    f"{result.sharpe_ratio:.2f}" if result.sharpe_ratio != 0 else "N/A",
                )

            console.print(detail_table)

        finally:
            await db.close()

    asyncio.run(_backtest())
```

### Integration Test Required

**File to create:** `tests/integration/cli/test_backtest_integration.py`

```python
"""Integration tests for backtest CLI command.

These tests verify that the backtest command uses REAL data, not mock output.
"""

from __future__ import annotations

import subprocess
import tempfile
from pathlib import Path

import pytest

pytestmark = [pytest.mark.integration]


class TestBacktestCLI:
    """Test that backtest CLI produces real, varying output."""

    def test_backtest_output_changes_with_date_range(self, tmp_path: Path) -> None:
        """Output MUST change when date range changes (proves not hardcoded)."""
        db_path = tmp_path / "test.db"

        # Initialize DB
        subprocess.run(
            ["uv", "run", "kalshi", "data", "init", "--db", str(db_path)],
            check=True,
            capture_output=True,
        )

        # Run backtest with two different date ranges
        result1 = subprocess.run(
            [
                "uv", "run", "kalshi", "research", "backtest",
                "--start", "2024-01-01",
                "--end", "2024-06-30",
                "--db", str(db_path),
            ],
            capture_output=True,
            text=True,
        )

        result2 = subprocess.run(
            [
                "uv", "run", "kalshi", "research", "backtest",
                "--start", "2024-07-01",
                "--end", "2024-12-31",
                "--db", str(db_path),
            ],
            capture_output=True,
            text=True,
        )

        # If both outputs are identical AND contain numbers, it's probably mock data
        if "10" in result1.stdout and "60.0%" in result1.stdout:
            if result1.stdout == result2.stdout:
                pytest.fail(
                    "MOCK DATA DETECTED: Output identical for different date ranges. "
                    "The backtest command is returning hardcoded fake results!"
                )

    def test_backtest_with_no_data_shows_appropriate_message(
        self, tmp_path: Path
    ) -> None:
        """Should show 'no settlements' message, not fake results."""
        db_path = tmp_path / "empty.db"

        # Initialize empty DB
        subprocess.run(
            ["uv", "run", "kalshi", "data", "init", "--db", str(db_path)],
            check=True,
            capture_output=True,
        )

        result = subprocess.run(
            [
                "uv", "run", "kalshi", "research", "backtest",
                "--start", "2024-01-01",
                "--end", "2024-12-31",
                "--db", str(db_path),
            ],
            capture_output=True,
            text=True,
        )

        # Should NOT show fake successful results
        assert "Total Trades" not in result.stdout or "0" in result.stdout, (
            "Empty database should not produce positive trade counts"
        )

    def test_backtest_parameters_are_actually_used(self, tmp_path: Path) -> None:
        """Verify parameters affect output (not just accepted and ignored)."""
        db_path = tmp_path / "test.db"

        subprocess.run(
            ["uv", "run", "kalshi", "data", "init", "--db", str(db_path)],
            check=True,
            capture_output=True,
        )

        # The date range should appear in the output
        result = subprocess.run(
            [
                "uv", "run", "kalshi", "research", "backtest",
                "--start", "2024-03-15",
                "--end", "2024-09-22",
                "--db", str(db_path),
            ],
            capture_output=True,
            text=True,
        )

        # The specific dates should appear in output (proves they're being used)
        assert "2024-03-15" in result.stdout or "2024-09-22" in result.stdout, (
            "Date parameters should appear in output, proving they're used"
        )


class TestMockDataDetection:
    """Meta-tests to catch mock data patterns."""

    def test_no_hardcoded_mock_comments_in_cli(self) -> None:
        """CLI should not contain '# Mock' or '# for now' comments."""
        import re

        cli_path = Path("src/kalshi_research/cli.py")
        content = cli_path.read_text()

        mock_patterns = [
            r"#\s*[Mm]ock",
            r"#\s*for now",
            r"#\s*placeholder",
            r"#\s*stub",
            r"#\s*fake",
            r"#\s*hardcoded",
        ]

        for pattern in mock_patterns:
            matches = re.findall(pattern, content)
            if matches:
                pytest.fail(
                    f"Found mock/placeholder comment in cli.py: {matches}. "
                    "This indicates unfinished implementation masquerading as complete."
                )

    def test_cli_commands_use_their_imports(self) -> None:
        """
        Commands should actually use the classes they import.

        If ThesisBacktester is imported but not instantiated, that's suspicious.
        """
        import ast
        from pathlib import Path

        cli_path = Path("src/kalshi_research/cli.py")
        content = cli_path.read_text()
        tree = ast.parse(content)

        # Find all imports of implementation classes
        implementation_classes = {
            "ThesisBacktester",
            "CorrelationAnalyzer",
            "MarketScanner",
            "AlertMonitor",
        }

        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom):
                for alias in node.names:
                    if alias.name in implementation_classes:
                        # This class is imported somewhere - verify it's instantiated
                        class_name = alias.name
                        # Simple check: is it called anywhere?
                        if f"{class_name}(" not in content:
                            pytest.fail(
                                f"{class_name} is imported but never instantiated. "
                                "This may indicate a mock implementation that imports "
                                "the real class but doesn't use it."
                            )
```

---

## Issue #2: Alerts Daemon Mode Not Implemented (ðŸŸ¡ MEDIUM)

### The Problem

**File:** `src/kalshi_research/cli.py`
**Line:** 1053

The `--daemon` flag is accepted but explicitly marked as not implemented:

```python
daemon: Annotated[
    bool, typer.Option("--daemon", help="Run in background (not implemented)")
] = False,
```

When used:
```python
if daemon:
    console.print(
        "[yellow]Warning:[/yellow] Daemon mode not yet implemented, running in foreground"
    )
```

### The Fix (Low Priority)

Option A: Remove the flag entirely until implemented
Option B: Implement using process backgrounding

```python
# Option A: Remove the flag
# Just delete the daemon parameter and the if block

# Option B: Implement basic daemon mode
if daemon:
    import os
    import sys

    # Fork to background
    pid = os.fork()
    if pid > 0:
        console.print(f"[green]Alert monitor started in background (PID: {pid})[/green]")
        sys.exit(0)

    # Redirect stdout/stderr to log file
    log_file = Path("data/alert_monitor.log")
    sys.stdout = log_file.open("a")
    sys.stderr = sys.stdout
```

---

## Issue #3: Order Placement Not Implemented (ðŸŸ¡ MEDIUM)

### The Problem

No order placement methods exist in the API client. This is **not** mock data (no fake implementation exists), but a missing feature.

### The Fix

**File:** `src/kalshi_research/api/client.py`

Add to `KalshiClient` class:

```python
async def create_order(
    self,
    ticker: str,
    side: Literal["yes", "no"],
    action: Literal["buy", "sell"],
    count: int,
    type: Literal["limit", "market"] = "limit",
    yes_price: int | None = None,
    no_price: int | None = None,
    expiration_ts: int | None = None,
    client_order_id: str | None = None,
) -> dict[str, Any]:
    """
    Place a new order.

    Args:
        ticker: Market ticker (e.g., "KXBTC-25JAN-T100000")
        side: "yes" or "no"
        action: "buy" or "sell"
        count: Number of contracts
        type: "limit" or "market"
        yes_price: Limit price in cents (1-99) for YES side
        no_price: Limit price in cents (1-99) for NO side
        expiration_ts: Unix timestamp for order expiration (optional)
        client_order_id: Client-provided order ID for idempotency

    Returns:
        Order response with order_id and status

    Raises:
        KalshiAPIError: If order placement fails
    """
    payload: dict[str, Any] = {
        "ticker": ticker,
        "side": side,
        "action": action,
        "count": count,
        "type": type,
    }
    if yes_price is not None:
        payload["yes_price"] = yes_price
    if no_price is not None:
        payload["no_price"] = no_price
    if expiration_ts is not None:
        payload["expiration_ts"] = expiration_ts
    if client_order_id is not None:
        payload["client_order_id"] = client_order_id

    return await self._auth_post("/portfolio/orders", payload)

async def cancel_order(self, order_id: str) -> dict[str, Any]:
    """Cancel an existing order by ID."""
    return await self._auth_delete(f"/portfolio/orders/{order_id}")

async def amend_order(
    self,
    order_id: str,
    count: int | None = None,
    yes_price: int | None = None,
    no_price: int | None = None,
) -> dict[str, Any]:
    """Amend an existing order's count or price."""
    payload: dict[str, Any] = {}
    if count is not None:
        payload["count"] = count
    if yes_price is not None:
        payload["yes_price"] = yes_price
    if no_price is not None:
        payload["no_price"] = no_price

    return await self._auth_post(f"/portfolio/orders/{order_id}/amend", payload)
```

---

## Issue #4: WebSocket Not Implemented (SPEC-014)

**Status:** SPEC ONLY - No code exists
**Priority:** P0 for performance, but not mock data (honestly documented as "Proposed")

See `docs/_specs/SPEC-014-websocket-real-time-data.md` for full specification.

---

## Issue #5: Rate Limit Tiers Not Implemented (SPEC-015)

**Status:** SPEC ONLY - No code exists
**Priority:** P1, but not mock data (honestly documented as "Proposed")

See `docs/_specs/SPEC-015-rate-limit-tier-management.md` for full specification.

---

## What's Actually Working (Verified)

These features are **genuinely implemented**, not mock:

1. âœ… **Portfolio Sync** - Real API calls, real FIFO cost basis, real mark prices
2. âœ… **Demo Environment** - Real URL switching via config
3. âœ… **Market Scanner** - Real data analysis
4. âœ… **Alert Monitor** - Real condition checking (just no daemon mode)
5. âœ… **Thesis Tracking** - Real JSON persistence
6. âœ… **Data Collection** - Real API pagination, real DB writes
7. âœ… **Calibration Analysis** - Real Brier score calculations
8. âœ… **Bug Fixes (1-39)** - Verified against code

---

## Priority Order for Implementation

1. **ðŸ”´ P0: Wire Backtest CLI** - CRITICAL. Mock data in production. (~30 min)
2. **ðŸŸ¡ P1: Add Order Placement** - Missing feature, enables trading (~2-4 hours)
3. **ðŸŸ¡ P1: Rate Limit Tiers** - Prevents API lockouts (~2-4 hours)
4. **ðŸŸ¢ P2: WebSocket** - Performance optimization (~8-16 hours)
5. **ðŸŸ¢ P3: Daemon Mode** - Nice to have (~1 hour)

---

## How to Verify Fixes Are Real

After any fix, run:

```bash
# 1. Check for mock comments
grep -rn "# [Mm]ock\|# for now\|# placeholder" --include="*.py" src/

# 2. Run the integration tests
uv run pytest tests/integration/cli/test_backtest_integration.py -v

# 3. Manual verification: run with different inputs, expect different outputs
uv run kalshi research backtest --start 2024-01-01 --end 2024-06-30
uv run kalshi research backtest --start 2024-07-01 --end 2024-12-31
# ^^^ These MUST produce different output
```

---

## Audit Methodology

Used to find these issues:

```bash
# Find explicit mock comments
grep -rn "# [Mm]ock\|# for now\|# placeholder\|# stub" --include="*.py" src/

# Find hardcoded table output
grep -rn "add_row.*\"\d" --include="*.py" src/

# Find TODO/not implemented
grep -rn "TODO\|FIXME\|not implemented" --include="*.py" src/

# Find suspicious static returns
grep -rn "return 0$\|return \[\]$\|return {}$" --include="*.py" src/

# Cross-reference: find classes that exist but CLI doesn't use
# 1. Find implementation classes
grep -rn "class.*Backtest\|class.*Scanner\|class.*Analyzer" --include="*.py" src/
# 2. Check if CLI imports and instantiates them
grep -rn "ThesisBacktester(" --include="*.py" src/
```

---

**Summary:** The previous agent wasn't systematically reward hacking - most fixes are real. But the backtest CLI mock data is egregious and must be fixed immediately. The CODE_AUDIT_CHECKLIST.md now has this pattern at the TOP to prevent recurrence.
