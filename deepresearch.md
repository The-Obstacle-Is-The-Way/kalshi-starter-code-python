Kalshi Research Platform – Architecture Evolution Plan
Architecture Recommendation (Single vs. Multi‑Agent)
For the goal of “surfacing mispriced prediction markets using AI research,” a single-agent architecture is the prudent starting point. The DeepMind study indicates that adding agents can hurt tool-heavy, sequential tasks. In fact, multi-agent setups showed 39–70% performance degradation on sequential reasoning tasks[1], which fits our use-case (gather data → analyze → synthesize). A single capable agent using tools sequentially will avoid the coordination overhead that multi-agent teams impose on tool usage[2]. In short, one agent can drive the entire workflow (calling APIs, searching news, querying the database, etc.) without the latency or error-amplification of handing off between agents.
If we later find clear sub-tasks that benefit from parallelization (e.g. fetching independent data sources concurrently), we can introduce a centralized multi-agent pattern. The DeepMind paper suggests a central “Supervisor” agent delegating to specialist sub-agents can preserve quality better than agents working independently[3]. In a centralized scheme, a primary agent would coordinate specialized helpers (for research, history, arbitrage), then integrate their outputs. This avoids the 17× error amplification seen in fully independent agents[3]. However, initial implementation will stay single-agent until there’s a justified need for concurrency or specialization. This aligns with the principle “start simple, add complexity only when justified.” We expect a single agent (using structured tools) to achieve a strong baseline – per the paper, coordination only yields gains if the single-agent performance is low (below ~45% task success)[2]. Given modern model capabilities, one agent will likely suffice at first. In summary, optimize for a single orchestrator agent now for reliability, and keep the door open for a centralized multi-agent upgrade if scaling demands it.
Agent Design & Coordination Pattern
If/when we move beyond one agent, the design will follow a centralized “hub-and-spoke” pattern rather than peer-to-peer voting or independent agents. The task naturally breaks into stages, which map to potential agents: - Research Agent – uses the Exa API (or similar) to fetch and summarize current news and insights relevant to a market. - Historical Analysis Agent – performs retrieval-augmented lookup of past similar markets (using embeddings) and computes any statistical baselines or patterns. - Arbitrage/Consistency Agent – checks logical consistency or cross-market arbitrage opportunities (e.g. related markets that should constrain each other’s probabilities). - Judge/Synthesizer Agent – acts as the central coordinator, validating inputs and ultimately producing the final probability estimate with reasoning.
In a centralized coordination, the Judge agent would sequentially call upon the others (either in parallel or series as appropriate) and aggregate their findings into the final output. This pattern aligns with recommendations that a supervisor can contain error propagation to ~4× instead of 17×[3] and improve performance significantly on tasks that can be parallelized[4]. For example, the Judge could prompt the Research and Historical agents in parallel (since gathering news vs. historical data are independent), then wait to combine their outputs. Parallel execution could speed up response time – something Pydantic Graph supports via parallel nodes – but it should be introduced only if latency becomes an issue. The Pydantic AI framework doesn’t implicitly spawn multiple agents on its own; instead it provides patterns for us to orchestrate them if needed[5]. We can implement agent delegation (one agent calling another as a tool), programmatic hand-off (run one agent then pass control to the next via code), or define an explicit workflow graph via pydantic-graph if the logic gets complex[5]. This gives us flexibility to gradually layer in agents.
Initial approach: Keep everything in a single agent’s chain-of-thought. The single agent (think of it as the Judge doing all tasks itself) can call tools/functions for API queries, database fetches, and Exa search. This avoids overhead while our volume is low. Only if performance bottlenecks or accuracy issues emerge (e.g. the agent’s context becoming too large or domain expertise too varied) will we instantiate distinct agents. At that point we’d use the centralized model – a top-level orchestrator agent explicitly coordinating specialist agents – rather than letting multiple agents free-roam. We will avoid decentralized “debate” patterns unless a clear need arises, since the research found peer agents offer little benefit on our type of problem[6]. In summary, one agent with tools now; centralized multi-agent if needed later, ensuring any added agents serve a well-defined purpose (and are orchestrated to minimize added error).
Step-by-Step Implementation Plan
To replace Claude Code’s non-deterministic orchestration with a structured, maintainable pipeline, we will implement the following steps:
Step 1: Create a Structured Analysis CLI Command. We will introduce a new CLI entry point (e.g. kalshi analyze or kalshi predict) that runs the end-to-end analysis for a given market and outputs structured JSON. This command will perform all the necessary sub-tasks in code: fetch market data from the Kalshi API/DB, call Exa for research, optionally retrieve historical stats, and then call an LLM to synthesize the final probability and reasoning. By consolidating the logic in one CLI command, we let our code (not the external AI) orchestrate the sequence deterministically. The CLI will ensure JSON out – for example, it might print a JSON object like:
{  "market_id": "XYZ123",  "market_question": "...",  "predicted_prob": 73,  "confidence": "medium",  "reasoning": "...",  "factors": [ ... ],  "sources": [ "https://news.example/... ", ... ]}
Claude Code (or any caller) can then consume this structured output directly. This immediately eliminates the “LLM is vibing” problem – the heavy lifting moves inside our controlled environment.
Step 2: Define Pydantic Schemas for Data Flow. All inputs/outputs between components will be defined as Pydantic models to enforce structure and validation. We’ll create models such as: - MarketInfo – fields for market ID, question text, current order book or odds, volume, etc (populated from the Kalshi API). - NewsArticle (for Exa results) – e.g. { title: str, url: HttpUrl, summary: str, relevance: float }. - ResearchSummary – a structured summary of factors affecting the market (e.g. list of key Factor objects each with a description and source reference). - HistoricalStats – e.g. { similar_events_found: int, base_rate: float, details: [...] } summarizing any historical pattern (could include distribution of outcomes for similar past events). - AnalysisResult – the final output schema with fields like predicted_prob: int (0–100), confidence: Literal["low","medium","high"], reasoning: str (a concise justification), and sources: List[HttpUrl]. This model may also nest the above components or include them for traceability (e.g. include the top factors from research and a snippet of historical stats in the reasoning).
Using Pydantic here enforces that each step’s output conforms to an expected schema. If the LLM or any function produces invalid data, we’ll catch it immediately (no silent hallucinations). These schemas will be reused as we evolve the system – for example, the AnalysisResult model today will remain the output contract for an agent in the future.
Step 3: Integrate Instructor for LLM Structured Output. We will use the Instructor library to call the LLM such that it returns a Pydantic model instance directly. Instructor allows us to “get reliable JSON from any LLM”, built on Pydantic validation[7]. In our new kalshi analyze command, instead of crafting an open-ended prompt and parsing text, we’ll do something like:
from instructor import from_provider# Define Pydantic model AnalysisResult aboveclient = from_provider("google/gemini-flash")  result: AnalysisResult = client.chat.completions.create(    response_model=AnalysisResult,    messages=[ ...constructed system/user prompts with market info and data... ])
This will prompt the model to output a JSON adhering to AnalysisResult schema, and Instructor will handle parsing and validation (no manual JSON parsing or error-prone string munging). If the model’s response doesn’t parse into the schema, Instructor can retry or throw an error – ensuring we don’t proceed with malformed data[8]. This structured LLM call replaces Claude’s fuzzy synthesis with a deterministic, validated step. We’ll start with a cheap but capable model (e.g. Gemini Flash or GPT-4o-mini) for this, as these are cost-effective ($0.15–$0.60 per 1M tokens) and should handle the task with proper prompting. Instructor keeps things “schema-first… simple and cheap,” and we’ll only reach for the heavier PydanticAI agent runtime when needed[8].
Step 4: Orchestrate with Claude Code (Transitional). In the immediate term, we’ll still use Claude Code CLI as the external orchestrator, but its role will simplify drastically. Rather than issuing a whole sequence of low-level commands and trying to interpret them, it can now do something like: human: "Find mispriced markets today" → Claude Code: runs 'kalshi analyze --all-markets' → gets JSON results → maybe formats a nice summary to present. Essentially, Claude becomes a thin wrapper that triggers our structured pipeline and perhaps translates the JSON into a human-friendly report. The key improvement is that Claude is no longer reasoning about how to combine data; it just trusts our CLI’s output. This eliminates hallucination in the synthesis step. We should ensure Claude’s prompt instructs it to use the JSON output as authoritative. In this transitional phase, Claude acts as a scheduler/interface, not as the brains of the operation.
Step 5: Upgrade to Internal Agents Gradually. Once the structured CLI pipeline is working reliably, we can begin migrating the orchestration inside the application using PydanticAI. The idea is to eventually remove the dependency on an external agent (Claude Code) and let our own code handle multi-step reasoning with LLM calls. PydanticAI will allow us to turn what was one monolithic kalshi analyze prompt into a more interactive agent if needed, with the same Pydantic models ensuring consistency. The migration can happen in stages: - Initially, we can keep the single-step approach (the CLI already calls one model via Instructor). This is already using an LLM internally, so in effect we have embedded the “agent” inside. - Next, if we want more complex chains (multiple tool calls interwoven with LLM reasoning), we can refactor the CLI logic using Agent classes from PydanticAI. For example, we might create a KalshiAnalystAgent that has tool functions registered for get_market_data, exa_search, etc., and let it reason through the steps. This agent would use the same models and could be executed via a simple agent.run(query) call inside our CLI (or via a future web UI). - As this grows, we might introduce the multi-agent patterns discussed (delegation or a Pydantic Graph workflow). Crucially, all the code we write now (CLI commands, functions for data retrieval, Pydantic models) will be reusable in that setup – nothing is throwaway. For instance, the function behind kalshi research exa (if we make one) could later be invoked as a tool by an agent instead of via subprocess.
Step 6: Introduce Pydantic Graph for Complex Orchestration (if needed). If our pipeline eventually needs to branch or parallelize tasks, we will adopt pydantic-graph to formalize the workflow. Pydantic Graph lets us define nodes (which can be our CLI sub-commands or agent calls) and edges using type hints, to create an async execution graph[9]. We would reach for this only when we outgrow simpler sequencing. For example, if we want to concurrently evaluate 100 markets every hour, a graph could spawn multiple agents in parallel or branch the workflow by market category. At that scale, we might also integrate with scheduling/backpressure systems (Pydantic Graph has integrations for Temporal, etc., but that’s beyond MVP scope). The key is our architecture choices now (Pydantic models, modular commands) keep us ready for this step. If one day mispricing detection becomes complex enough to justify many agents coordinating, we could even consider LangGraph – but given our scope and DeepMind’s findings, that is unlikely in the near term. Simpler PydanticAI graph orchestration should suffice.
By following these steps, we achieve a minimally viable structured pipeline quickly (Steps 1–4), solve the immediate synthesis problem, and lay the groundwork to scale up (Steps 5–6) without painful rewrites. The plan emphasizes incremental evolution: first get deterministic outputs with one agent, then add sophistication as needed.
Pydantic Schema Definitions
To ensure structured I/O everywhere, we will define clear data models for each component’s input and output. Below are the key Pydantic schema definitions that will drive the system (fields subject to refinement as we implement):
	•	MarketInfo – Represents basic market data pulled from Kalshi. Fields: market_id: str, question: str, category: str, current_price: float (or odds/probability), volume: float, expiry_date: datetime, etc. (This can be populated via the Kalshi API or our database snapshots.)
	•	NewsArticle – Represents a single search result from Exa (or other news source). Fields: title: str, url: HttpUrl, published_date: datetime | None, snippet: str (short summary or excerpt), relevance_score: float | None. (We might get a relevance ranking from Exa or compute it based on query match.)
	•	ResearchSummary / Factor – Summarizes the findings from the research agent or Exa results. Likely a list of Factor objects, where each factor has description: str (e.g. “Federal Reserve hinted at rate hike, which may affect markets”), impact: str | None (optional qualitative impact, e.g. “could drive probability down”), and source: HttpUrl (a reference to a source supporting this factor). The ResearchSummary could then be a model containing factors: List[Factor] and perhaps an overall summary_text: str. This structure forces the AI to output concrete factors with sources, aiding traceability.
	•	HistoricalStats – Captures any insights from past data. Fields: For example, base_rate: float (historical frequency of the event outcome = True), similar_cases: List[HistoricalCase]. Each HistoricalCase might include event_description: str, outcome: bool or outcome_prob: float, date: datetime, and maybe an similarity_score: float. These would come from our RAG pipeline once implemented. Initially, we might leave this out or stub it with an empty list if RAG isn’t ready. But defining it now means when we add RAG, it slots into the output schema without changing the interface.
	•	ArbitrageCheck – (If we include arbitrage logic) Could contain fields like related_market_id: str, discrepancy: str (e.g. “Market X implies this should be 10% higher”), or a list of any inconsistencies found. This is optional, and might be rolled into the reasoning text rather than a separate structure at first. It’s something to consider as an extension.
	•	AnalysisResult – Final output model of the system. Fields:
·      market_id: str (to tie back to which market was analyzed),
·      predicted_prob: int (the percent probability the event will resolve YES, 0–100),
·      confidence: Literal['low','medium','high'] (qualitative confidence in the estimate),
·      reasoning: str (a concise explanation, potentially incorporating factors and historical context in narrative form),
·      factors: List[Factor] (carry over the key factors from ResearchSummary for transparency),
·      sources: List[HttpUrl] (all source URLs referenced in the reasoning/factors),
·      timestamp: datetime (when the analysis was performed, for logging).
All CLI commands and agents will interchange data in these forms. For example, a kalshi research command might output a JSON list of NewsArticle objects, which the analyze command (or agent) can accept as input. By using Pydantic’s .json() or model serialization, passing data between steps becomes safe and straightforward. Importantly, these schemas also enable observability – we can log complete AnalysisResult records for each run, accumulate statistics on accuracy later, and ensure we know exactly what inputs led to what outputs.
CLI Commands and Future Agents (Tooling)
Designing CLI commands that cleanly map to future agent tools is a priority. We’ll refactor or add commands such that each performs a single logical task with structured I/O. Proposed CLI commands (and how they translate to agent tools/nodes):
	•	kalshi markets [--filters...] – (Possibly existing) Fetches markets from the Kalshi API or local DB. Output: JSON list of MarketInfo objects. This could accept filters (category, close-date range, etc.) to narrow scope. In an agent context, this would correspond to a Market Data Tool function (no AI, just data retrieval).
	•	kalshi scan – Scans all markets for anomalies or close arbitrage (likely already implemented in some form). We may adjust this to output structured results (list of markets or opportunities) instead of human-formatted text. This might not be directly used by the new pipeline, but it’s part of the toolset (could be used by an agent for a broad scan task).
	•	kalshi research <market_query> – Uses Exa (or other sources) to find relevant news/research for the given market question. Output: A JSON ResearchSummary (or list of NewsArticle) with the key findings. This encapsulates the Exa integration as a standalone step. Today, Claude Code could call this and then feed its output into the analyze step. In the future, we can register the underlying function as a Research Tool that an agent calls when it needs context. By keeping Exa logic in one place, any agent (or even a human operator) can reuse it easily.
	•	kalshi analyze <market_id> – The new command described in Step 1 of the plan. It would likely internally call markets to get the specified market info, call research to get news factors (or call Exa directly in-process), possibly load historical data, then invoke the LLM to produce the AnalysisResult. Output: JSON AnalysisResult for the market. This is the command that replaces the ad-hoc Claude synthesis. In agent terms, this whole command is essentially our Analyst Agent in single-agent mode. As we evolve, the logic inside might be broken into multiple agent calls, but externally it could remain one command for convenience (and backward compatibility for scripts or the human user).
	•	(Future) kalshi analyze --batch or a scheduler/daemon mode – In the future, we might add options to analyze multiple markets in one go (e.g. top 10 volume markets today) and either output an array of results or store them in the database. This isn’t needed immediately, but we anticipate it for scaling to daily scans. It’s essentially a wrapper that calls kalshi analyze repeatedly and aggregates results. We could also integrate this with an alerting mechanism if certain criteria are met (e.g. send an email if probability deviates significantly from baseline).
Each CLI command will be implemented with the mindset that it could later become a function callable by an agent. That means: - No interactive prompts (always output machine-readable data). - Idempotent and stateless (they rely on input args and global config/DB, not on hidden state in the CLI process). - Minimal side effects (e.g. the research command just returns data; it doesn’t, say, update the DB or require user input).
By adhering to these, when we transition to PydanticAI, we can directly reuse these as tool functions. For example, kalshi research may correspond to a Python function def research_market(query: str) -> ResearchSummary that we register with the agent. The same function can be invoked via CLI or by the agent internally. This satisfies the “no throwaway code” constraint and eases testing (we can unit test these functions/commands independently of the agent).
LLM Orchestration Strategy (Claude vs. Internal)
The question of Claude Code vs. internal LLM orchestration boils down to timing and control. We will adopt a hybrid approach: keep using Claude Code in the very short term to drive the CLI, but begin shifting the orchestration logic into our own code via structured LLM calls. Concretely:
	•	Immediate Term (Option A): Claude Code remains the top-level orchestrator, but it now invokes a single intelligent CLI command (kalshi analyze) rather than juggling multiple steps itself. This preserves the current setup where Claude acts like a free “human” running the tool, but drastically simplifies its job. Claude will basically just do: “User asked X → run the appropriate CLI command → return the result as answer.” The reasoning and synthesis will happen within our command using a deterministic prompt with Instructor. We prefer this now because it’s the path of least resistance – we leverage Claude to handle the user interaction, while our CLI ensures the heavy reasoning is structured. It lets us validate the new pipeline quickly while still benefiting from Claude’s natural language interface for any glue or final formatting.
	•	Next Phase (Option B): Embed the full agent loop inside the CLI, phasing out reliance on Claude Code. Once we have confidence in our kalshi analyze pipeline, we can move to calling the LLM API directly from our code (which we’re already starting to do with Instructor) and even handle multi-step reasoning internally. At that point, Claude Code is no longer needed to chain commands – our application can be run headless or on a schedule, producing outputs without an external brain. For example, we could have a simple script or cron job that runs kalshi analyze --batch every morning and emails results, with no GPT/Claude in the loop except the calls our code makes via Instructor/PydanticAI. This eliminates nondeterminism completely and also frees us from constraints of the Claude CLI interface. Option B is essentially achieved by gradually expanding the intelligence of our CLI using PydanticAI (as described in the Implementation Plan steps 5–6).
	•	External Orchestrator “Something Else” (Option C): We should avoid introducing any new external orchestration layer (like another agent to manage agents) – that would add complexity we don’t need. The Claude Code CLI is already fulfilling that role, and as we improve the internals, we’ll simply outgrow the need for it. The ideal end-state is: our system runs itself (perhaps triggered by a scheduler or minimal wrapper script), using PydanticAI for orchestration and Instructor for structured outputs at each step. This keeps all logic and verification in our hands.
In summary, the plan is to use Option A now, and transition to Option B over time. We’ll keep Claude Code around as a helpful scaffolding while we implement and test the structured pipeline (it’s essentially a free UI and natural language layer). But the ultimate architecture will have the orchestration happening within the app, because that’s where we can enforce type-checks, add proper logging, and reduce hallucination to zero. Each step toward internal orchestration will be done in a backward-compatible way – we can always allow a mode where a human or Claude triggers the analysis – but the core will no longer rely on an intelligent external agent doing decision-making. This strategy gives us the best of both worlds: immediate functionality with minimal changes (Claude + one new command), and a clear path to a fully autonomous, robust system (PydanticAI-driven) when we’re ready.
Volume & Frequency Scaling Strategy
Right now, you trigger analyses manually (ad-hoc when time permits). The architecture should accommodate scaling up to daily or continuous monitoring without a complete redesign. Here’s the approach to handle increasing volume gracefully:
	•	Design for Idempotent Re-runs: All components (API calls, research queries, analysis prompts) should be repeatable and not rely on unstated context. By using fixed input -> structured output functions, we ensure running the process daily or hourly yields consistent results for the same state of the world. This means if we accidentally run it twice, it either produces the same result or updates the same database entries without duplication. Idempotency and statelessness (outside the DB) will make it safe to increase frequency.
	•	Manual to Automated Transition: We will start with manual or on-demand runs (perhaps initiated via the CLI or Claude interface), but keep the option to automate open. For example, once the kalshi analyze command is stable, we can set up a simple cron job or scheduled GitHub Action to run kalshi analyze --batch every day at a certain time. Because the output is structured (e.g. JSON or a stored file), the job could email a report or commit results somewhere. This doesn’t require any new architecture – it’s just operationalizing what we already have.
	•	Avoid Premature Optimization: We explicitly want to avoid over-engineering for scale we don’t yet need. So, we won’t immediately build a complex message queue or streaming system. Instead, we leverage the fact that a single-agent approach can sequentially handle a few hundred markets in a reasonable time (especially if using efficient APIs and caching where possible). Our stack (Python + SQLite + some API calls) can likely scan hundreds of markets in minutes. That’s sufficient for daily or even multiple-times-daily runs. If later we need near-real-time monitoring of thousands of events, that’s when we’d consider scaling out – e.g. using multiple threads or agents in parallel, more robust databases, etc. But for now, simplicity is king. A cron triggering the existing pipeline is often the simplest scalable solution.
	•	Resource and Rate Limit Considerations: As we increase frequency, we’ll watch out for external limits. The Kalshi API and Exa API likely have rate limits or cost implications. We should implement basic rate limiting and caching. For instance, if we scan the same market multiple times a day, cache its info for a short period instead of hitting the API every time. Similarly, if our Exa queries repeat (e.g. the same query daily until market closes), perhaps only do a deep search once daily and reuse it unless something changes. These optimizations can be gradually added when moving to continuous mode.
	•	Future Daemon Mode: In an ideal future where opportunities need instant reaction, we might run the system as a daemon (an always-on process) that listens for new events or price changes and triggers analyses. Our architecture will be ready for that because we’re keeping it modular. For example, one could imagine a small loop that checks the order book every X minutes and calls our analysis function if a big change is detected. This would essentially embed our pipeline into a persistent service. We won’t build that today, but nothing in our design precludes it. By using Pydantic models and robust logging, even a continuously running agent can be monitored and trusted.
Bottom line: We will start manual but write code as if it will be automated. This means careful handling of state, errors, and idempotency. Scaling to daily runs is mostly an ops consideration (deploying a scheduler) once the code is solid. We won’t prematurely introduce distributed systems or multi-agent concurrency just for volume – not until it’s clearly necessary (if ever). This way we don’t paint ourselves into a corner; we ensure the pipeline can grow from a single on-demand run to a scheduled frequent task smoothly.
Retrieval-Augmented Generation (RAG) Considerations
Integrating a RAG pipeline for historical data is an enticing idea, but we should weigh its immediate value against complexity. Currently, Exa’s real-time search might cover most of our needs for finding relevant information (news, expert commentary, etc.). RAG would primarily help incorporate internal or historical knowledge that isn’t readily available via a search query. Here’s how we’ll approach it:
	•	Phase 1 – Validate Core Loop Without RAG: First, get the core mispricing detection working using just live data (Kalshi API, Exa search) and the LLM’s reasoning. This will prove the concept. We’ll likely discover whether the LLM is lacking historical context or not. For example, if the analysis output often says “I’m not sure how similar events played out” or makes obvious mistakes that historical data could have prevented, that’s a signal to prioritize RAG next.
	•	Phase 2 – Minimal Viable RAG (Historical Base Rates): If we decide RAG will add value, start with a simple integration: use the HuggingFace kalshi-prediction-markets-betting dataset (5M+ trades) or our own records of past markets to derive some baseline probabilities. One straightforward tactic is to compute the base rate: e.g., “In past 50 markets about elections, 38% resolved Yes.” Even without fancy embeddings, we can use metadata (category, keywords) to find roughly similar past events. This could be done with a simple search or filter on a curated dataset of resolved markets. The result can feed into the reasoning as an additional data point (“Historically, similar questions had ~38% chance”). This gives the LLM a prior to adjust from.
	•	Phase 3 – Embedding-based Similarity Search: For a more nuanced RAG, we’ll employ embeddings to find truly similar events or scenarios. By 2026, embedding models have advanced but OpenAI’s text-embedding-ada-002 (from 2022) remained a strong baseline and likely successors or open equivalents (from Cohere, Meta, etc.) are available. The best practice is still to use a high-dimensional transformer-based embedding that captures semantic meaning. We could use OpenAI’s latest if allowed, or an open model like Cohere’s or InstructorXL embeddings. The key is consistency – we’ll embed all past market descriptions and outcomes with the same model for comparison.
	•	Vector Store Choice: To store embeddings, a lightweight choice like ChromaDB or a simple FAISS index on disk works well initially. Since we already use SQLite, we might even try the SQLite Vector extension or pgvector if we move to Postgres. These let us avoid running separate infrastructure. Given the dataset size (5M trades, but unique markets will be far fewer), this is manageable locally (perhaps a few thousand resolved markets to embed). For development, we can start with a subset (e.g. last 1-2 years of markets) to keep it quick.
	•	Granularity of Chunks: We need to decide what we embed. Likely, each market/event is one data point – a vector representing the event description + context (maybe concatenating the market question and a brief summary of outcome or news around it). If we embed news articles or long texts, we’d chunk those by paragraph. But since Exa covers current news, RAG would be more about internal data: past events, and maybe our thesis notes. For thesis tracking (your own notes and predictions), we can absolutely embed each thesis entry as well, so the system can recall “You predicted X for a similar event last year and it went differently.”
	•	Integration into Pipeline: When analyzing a new market, we could take its description, use the embedding to query the vector store for the top N similar past events or notes, and then include those findings in the LLM prompt. For example: “Past Event: ‘Fed raised rates in 2024’ – outcome Yes, market was mispriced by 10%.” The LLM can then consider these analogies in its reasoning. This effectively grounds the prediction in historical precedent.
	•	Now vs. Later: Our inclination is to defer a full RAG implementation until the core system is proven. We will, however, design the system such that adding RAG is straightforward. This means defining the HistoricalStats model as mentioned, and perhaps scaffolding a function or agent for retrieving historical info (even if it returns none or dummy data initially). Once the primary loop is working, we can implement that function to do the embedding lookup and fill the model. Deferring RAG avoids premature complexity, but keeps the door open.
	•	Exa vs. RAG: Note that Exa’s Deep Research API might blur the line – if their “deep research” endpoint pulls historical context or does analysis, it could supplement or even obviate some RAG needs. However, relying on Exa for historical patterns might be limited; likely it’s more focused on current info. Our own RAG would uniquely allow incorporating proprietary data (like your personal thesis logs or trades dataset). So, Exa is for broad external knowledge, RAG is for in-house knowledge.
To summarize, RAG is not strictly necessary on day one, given the system’s scope, but it is a powerful enhancement to plan for. We’ll start by logging where the LLM might have benefited from historical context. As soon as we see clear opportunities (e.g. a repeated question type where an empirical frequency would help), we’ll implement a focused RAG solution for that. Using modern embedding tech and a simple vector DB, this is quite achievable. The outcome will be a more calibrated AI – not just reacting to the latest news, but also informed by the past.
Exa Search Integration Strategy
Incorporating Exa (the deep research search API) is a priority for getting real-time context on markets. We want to integrate Exa in a way that aligns with our structured, tool-based architecture:
	•	Encapsulate Exa as a Tool/Function: We will likely implement a CLI command (and corresponding Python function) dedicated to Exa queries – for example, kalshi research "<query>". This command will call the Exa API with the provided query string and retrieve results (news articles, research papers, etc.). The output will be structured, e.g. a list of NewsArticle objects or a ResearchSummary model as defined. By encapsulating it, we achieve two things: (1) We can use it in the pipeline now by having our analyze command call this under the hood, and (2) later, we can register the same function as an Agent tool so that a PydanticAI agent could invoke Exa whenever it needs more info on a topic.
	•	Standard Search vs. Deep Research: Exa likely offers a basic search endpoint (returning relevant documents/web results) and a more advanced “Deep Research” endpoint (perhaps returning synthesized reports or multi-hop analysis). Initially, we’ll use the standard search functionality to keep things transparent. That means our code gets raw search results (titles, snippets, maybe full text of top articles) and we then use our own LLM to summarize or extract what’s needed. This fits our requirement for traceability – we see each source and can cite it in the reasoning. The Deep Research API might do some of this for us (possibly using its own LLM), but that introduces another black-box LLM in the loop. We might experiment with it later (if it can produce, say, a pre-summarized report of findings), but to start, using standard search ensures we maintain consistent structured handling.
	•	Integration into the Analysis Flow: In the new kalshi analyze command, after retrieving the market question, we will call our Exa search function with a well-crafted query. Likely the query will be the market title plus keywords like the category or related entities (for example, if the market is “Will John Doe win the election?”, the query might be “John Doe election polling news”). We may need to prompt-engineer this query or use any advanced search filters Exa provides to get high-quality results. The search results (top 3-5 articles) will then be either passed directly into the final LLM prompt or first processed.
                 Option A: One-step LLM synthesis – Provide the LLM with the market info and the raw search snippets, and ask it to directly produce the probability and reasoning. The LLM would read the snippets and integrate them in one go. This is simpler, but we must keep snippet length reasonable to avoid context overflow. We can truncate or select the most relevant parts of each article.
                 Option B: Two-step LLM processing – First use a smaller prompt to summarize the search results into structured factors (using Instructor to output a ResearchSummary with key points), then feed those distilled factors into the final analysis prompt. This adds an extra LLM call but forces an explicit “facts gathering” step separated from “analysis” step. It could improve quality by ensuring the final reasoning isn’t overwhelmed with extraneous text. Given the DeepMind finding that tool-heavy tasks can suffer from too many agent steps[2], we might stick to one LLM call if possible for efficiency. But if the final agent context becomes too large, the two-step approach is a viable solution.
	•	Use via MCP or Directly: You mentioned Exa integration via MCP (Model-Context Protocol, presumably for tool plugins). In PydanticAI, we could integrate Exa as a function tool that the agent can call at will (similar to how one might integrate a browser tool). This is indeed the long-term vision: the agent could decide “I need more info” and call the exa_search() function mid-prompt. To do that, we’ll register the Exa function with the agent when setting up PydanticAI (once we get to that stage). For now, though, we don’t have an autonomous agent – we’re orchestrating manually – so we’ll call Exa in a fixed way in code. That’s absolutely fine for deterministic output. As soon as we shift to an agent that can reason about calling tools, we will include the Exa search as one of its available tools. We’ll reuse the same research_market(query) -> ResearchSummary function, just exposing it to the agent. The MCP client/server might allow an external orchestrator like Claude to call Exa through our system as well, but since we plan to internalize orchestration, our focus is making it a tool for our agent.
	•	Error Handling and Rate Limits: Integrating Exa means handling cases where the API might fail or return nothing. Our kalshi research command should handle exceptions (network errors, etc.) and perhaps return an empty result or a message that no info was found, rather than crashing. If Exa has a call quota, we should use it judiciously (e.g. don’t query the same thing repeatedly in short succession; perhaps cache results for a given query for some minutes). These considerations will be baked in so that using Exa doesn’t reduce our system’s reliability.
	•	Trade-off: By doing our own summarization of Exa results, we ensure the final explanation cites specific sources (as you desired: output has a sources list). This is a big plus for validation – you can trace why the model thinks a market is mispriced by looking at the linked evidence. We’ll instruct the LLM to explicitly reference those sources in its reasoning. If we had Exa’s “deep research” do all the summarizing, we might lose some transparency. So, initially sticking to basic search aligns with our traceability goals.
In conclusion, Exa will be integrated as a modular research component. Use a simple CLI function now (kalshi research) that returns structured info, and plan to register that as an agent tool later. This gives us the immediate benefit of rich external knowledge in our analysis, without entangling our architecture in knots. It also follows the paper’s guidance to be cautious with multi-agent overhead on tool use – we’re essentially using Exa as a single tool within one agent’s workflow, which is efficient.
Observability & Validation (Error Handling)
Ensuring we can trace and trust the system’s outputs is important, especially as we add AI components that can fail in odd ways. Our plan emphasizes structured logging and validation at every step:
	•	Pydantic Validation Everywhere: Because every data exchange uses Pydantic models, any anomaly (missing field, wrong type, out-of-bounds value) will throw a validation error. This is a feature, not a bug – it prevents bad data from propagating silently. For example, if the LLM somehow returns predicted_prob: "sixty percent" instead of an integer, Pydantic will flag it and we can catch that exception. In the CLI, we’ll wrap calls in try/except so that if validation fails, we log the error and possibly output an error message in JSON (or at least avoid using incorrect data). This gives us an early warning when something is off.
	•	Pydantic Logfire for Traceability: We intend to integrate Pydantic Logfire for observability. Logfire is built to work with Pydantic models and AI agents, providing a web UI to inspect runs[10]. By sending our execution logs to Logfire, we gain a live view of what the agent or CLI is doing: which tools were called, what inputs/outputs were at each step, and any validation issues. For now, since we have a single-agent (or single-command) flow, the logging is straightforward – we can log the final AnalysisResult of each run, along with key intermediate info (like “Fetched 5 news articles for query…”). As we transition to PydanticAI, Logfire can capture each agent action in detail automatically (tool calls, messages, etc.). This will be extremely useful for debugging when an output seems wrong – we can replay the chain of events that led there.
	•	Source Citations in Output: As mentioned, the AnalysisResult.sources field will explicitly list the URLs or identifiers of sources used. We’ll enforce that the LLM populates this (by designing the output schema and prompt accordingly). This means when you get a prediction that Market X is 73% likely, you also get something like sources: ["https://www.politico.com/article...", "internal:similar_event_2024"]. You can click those (in our interface, we’ll ensure they’re easily viewable) to verify the evidence. This not only builds trust but also helps debug if the AI misunderstood something (maybe the source doesn’t actually say what the AI claims). In effect, we’re building a minimal audit trail for each prediction.
	•	Result Logging and Performance Tracking: We will log every AnalysisResult to a persistent store – likely our SQLite database (or a separate file). This creates a history of predictions. Coupled with actual outcomes (which we can load later once events resolve), we can compute Brier scores and calibration over time (you already have calibration analysis code that can be repurposed here). This addresses the need to track accuracy. Over weeks and months, we’ll gather data on how well the AI is doing and identify bias (e.g. consistently overestimating probabilities). It also helps identify if changes to the system (like adding RAG or new agents) improve or hurt performance.
	•	Error Handling Strategy: In a production-like continuous run, we don’t want the whole system to crash because one sub-step failed. Our CLI commands will be written to handle exceptions gracefully:
·      If the Exa API fails or times out, the research function can return an empty ResearchSummary or a flag indicating failure. The final analysis LLM prompt can then proceed with whatever info is available (and note “(no recent news found)” or similar if needed). That way one component failing doesn’t zero out the entire analysis.
·      If the LLM call itself fails (API error or validation error due to too-confident output), we can implement a retry mechanism. Instructor might handle some of this internally (e.g. re-prompt if JSON invalid). We can also fall back to a simpler output or an error message in the JSON saying the analysis couldn’t be completed. That’s better than silent corruption.
	•	Logging these errors to Logfire (with alerts maybe) will let us know when something went wrong, even if the system recovers gracefully.
	•	Minimal Observability vs. Overkill: You asked what’s the minimal needed. In our case, minimal viable observability is: structured logs of inputs/outputs and source attributions. Pydantic Logfire is a quick win here – it’s uncomplicated and gives a UI for exactly those needs (live monitoring, log exploration, even alerting). We don’t need to build a custom dashboard yet. We also don’t need a full APM or tracing system since the scale is small (one user, one process). However, we will likely add some metrics collection: e.g. how long each step takes, how many tokens each LLM call uses (to monitor cost), etc. PydanticAI/Logfire might help here too by capturing token counts if configured. We can set simple thresholds (like if an analysis is taking too long or using too many tokens, log a warning). This helps ensure performance doesn’t silently degrade.
In short, we will know what the system is doing and why. Between Pydantic validation, explicit source logging, and using Logfire for introspection, we should have a solid understanding of each prediction made. This will let us iteratively improve the system (we can quickly pinpoint if, say, the Research agent is feeding misleading info, or if the LLM is ignoring the historical data, etc.). For now, this level of observability is sufficient. We can revisit more advanced needs (like audit trails for every intermediate prompt, or a user-facing explanation UI) if the project grows, but we won’t over-engineer that until needed.
Potential Pitfalls and Mitigations
As a solo developer new to this space, it’s wise to anticipate pitfalls and anti-patterns. Here are a few to watch out for, along with how our plan addresses them:
	•	Overcomplicating the Architecture Too Early: A classic mistake would be to dive into a complex multi-agent setup and new frameworks (PydanticAI, Graph, etc.) all at once. This can lead to a fragile system that’s hard to debug. We’re mitigating this by starting simple (essentially a single-agent pipeline) and only adding complexity (multiple agents, parallelism) when clearly justified. We should be skeptical of the “cool factor” of multi-agent systems – the DeepMind research provides a sobering reminder that more agents can hurt performance[1]. So we’ll implement the simplest architecture that works and measure its performance before iterating.
	•	Relying on Undocumented Magic: Another anti-pattern is leaning on the LLM to do too much in an uncontrolled way (we experienced this with Claude Code hallucinating). Our restructuring explicitly avoids this by using structured prompts and tools. Every LLM action is guided by a schema or a tool function, meaning less room for unpredictable behavior. We’ll still be cautious that the LLM can produce plausible nonsense – hence the emphasis on sources and validation. Whenever possible, have the LLM output data we can verify (like source URLs or numeric calculations) rather than free-form prose alone.
	•	Not Testing Components in Isolation: It’s easy to focus on the end-to-end and neglect unit testing of pieces, which can come back to bite us. We have 362+ unit tests already; we must extend that to new components. For example, we should write tests for the Exa integration (e.g. does kalshi research return well-formed output given a sample API response?), for the schema validation (e.g. does an obviously bad LLM output get caught?), and for the final analysis logic (perhaps using a mocked LLM that returns a known JSON to see if the CLI handles it). By testing each module (API layer, DB, research, analysis prompt) independently, we catch issues early. This is especially important before hooking up the real LLM, as tests can simulate edge cases.
	•	Ignoring Token Costs and Latency: With multiple LLM calls and tools, there’s a risk the system becomes slow or expensive. While we plan to use cheaper models and perhaps only one main LLM call per analysis, we should monitor how the cost scales if analyzing many markets or if the prompt grows too long. If we notice prompt length bloat (say we’re feeding too many news articles), we’ll need to prune inputs or summarize earlier. Likewise, if we move to a continuous monitoring, we might need to ensure we’re not spamming the LLM unnecessarily. One mitigation is using caching for expensive steps (e.g. cache Exa results for a while, cache embeddings for RAG so we don’t recompute them each time).
	•	Integration Hell (Too many new frameworks): We’re introducing Pydantic, Instructor, PydanticAI, possibly Pydantic Graph, and using external APIs. There’s a risk of spending more time fighting integration issues than solving the problem. To avoid this, we’ll integrate one at a time:
·      Pydantic/Instructor – relatively straightforward, just ensure we structure code to use them. We can get that working independently (e.g. a small script that uses Instructor to parse a dummy prompt into our model, to verify setup).
·      Exa API – test this in isolation with a simple call to ensure we handle auth, response parsing, etc.
	•	PydanticAI – when we move to it, start with a minimal agent that does something trivial (like echo a response) to make sure the environment is correct, then gradually port our logic into it. By going stepwise, we reduce the chance of being stuck with multiple unknowns. Also, since these are all Pydantic ecosystem, they should play nicely together (Instructor and PydanticAI are designed to be complementary[8]).
	•	State and Data Consistency: As we run analyses regularly, we need to ensure we don’t corrupt our database or state with partial updates. For example, if we decide to log results to the SQLite DB, ensure that if the process crashes mid-run, we don’t end up with half-written entries or locks. SQLite WAL mode is forgiving, but we should still use transactions for any DB writes. Also, manage concurrency – if we ever did run two instances at once (perhaps via cron overlap), they might conflict. We can avoid that by not scheduling overlapping runs or by implementing a simple lock (even a file lock) if needed in the future.
	•	Security and API keys: This might be outside pure architecture, but as an FYI: storing API keys (Kalshi, Exa) in env or config is fine, but be careful with logging – never log the keys or full private data. If we use Logfire, check that it’s not inadvertently logging sensitive info. Also, if the system ever has a front-end, ensure we don’t expose the raw keys there. Basic stuff, but easy to overlook.
	•	Regret #1 to avoid – Tight coupling to Claude: We already identified that having an external AI orchestrator was a crutch. By eliminating that, we avoid being tied to a specific provider and we gain determinism. It’s a good move not just for performance but for maintainability. We’ll ensure not to fall into a similar trap elsewhere (e.g. don’t make our code depend on some specific UI or environment – keep it standard so it can run anywhere).
	•	Regret #2 to avoid – Analysis Paralysis: With so many new technologies and possibilities (agents, graphs, RAG, etc.), there’s a risk of spending too long designing the “perfect” system and not shipping anything. We should guard against that by aiming to have a basic version running ASAP (even if it’s just analyzing one market at a time via a command). Real-world use will inform what refinements are truly needed. This agile approach will prevent the “big rewrite” syndrome later.
By being aware of these pitfalls, we can proactively mitigate them. The overarching theme is incremental, testable improvements. We’ll frequently ask, “Is this added complexity really needed now, or can we get by simpler?” and “How will I verify this works as intended?” Following the plan we’ve laid out, we focus on solving the current problem (structured, reliable analysis) in a robust way, while architecting for future extension. This way, as your project grows from a one-person weekend project to possibly a more automated service, the foundation will be solid and you won’t be stuck with major regrets or rewrites.

[1] [2] [3] [4] [6] [2512.08296] Towards a Science of Scaling Agent Systems
https://arxiv.org/abs/2512.08296
[5] Pydantic AI vs LangGraph: Features, Integrations, and Pricing Compared - ZenML Blog
https://www.zenml.io/blog/pydantic-ai-vs-langgraph
[7] [8] GitHub - 567-labs/instructor: structured outputs for llms
https://github.com/567-labs/instructor
[9] Graph - Pydantic AI
https://ai.pydantic.dev/graph/
[10] Fire Up Your Logging Needs with Pydantic Logfire - Kader Miyanyedi
https://kadermiyanyedi.medium.com/fire-up-your-logging-needs-with-logfire-6330d7a08dfe
